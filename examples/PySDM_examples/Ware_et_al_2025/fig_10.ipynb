{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![preview notebook](https://img.shields.io/static/v1?label=render%20on&logo=github&color=87ce3e&message=GitHub)](https://github.com/open-atmos/PySDM/blob/main/examples/PySDM_examples/Ware_et_al_2025/fig_10.ipynb)\n",
    "[![launch on mybinder.org](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/open-atmos/PySDM.git/main?urlpath=lab/tree/examples/PySDM_examples/Ware_et_al_2025/fig_10.ipynb)\n",
    "[![launch on Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-atmos/PySDM/blob/main/examples/PySDM_examples/Ware_et_al_2025/fig_10.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 10 from [Ware et al. 2025](https://doi.org/10.48550/arXiv.2509.05536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip --quiet install open-atmos-jupyter-utils\n",
    "    from open_atmos_jupyter_utils import pip_install_on_colab\n",
    "    pip_install_on_colab('PySDM-examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import scipy\n",
    "from open_atmos_jupyter_utils import show_plot\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = 4/3 * np.pi * 30.531e-6**3\n",
    "DV = 1e6 #m3\n",
    "N0 = 2**23 \n",
    "\n",
    "PARAMS_P = SimpleNamespace(\n",
    "    n_part=2**8,\n",
    "    norm=N0 * DV,\n",
    "    dist=scipy.stats.expon(loc=0, scale=X0),\n",
    ")\n",
    "\n",
    "PARAMS_T = SimpleNamespace(\n",
    "    span = 300,\n",
    "    n_step = 2,\n",
    "    b = 1500, #1/s\n",
    "    dv = DV\n",
    "    \n",
    ")\n",
    "\n",
    "PARAMS_T.step = PARAMS_T.span / PARAMS_T.n_step\n",
    "\n",
    "RNG = np.random.default_rng(seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(*, params_p, rng):\n",
    "    \"\"\" samples a particle population specified onto a grid and returns\n",
    "    a collection indexed by sampling type, with values composed of `size` and `mult`\n",
    "    real- and integer-valued arrays, respectively \"\"\"\n",
    "    u01 = rng.uniform(0, 1, size=params_p.n_part)\n",
    "    x_range = params_p.dist.ppf(.0001), params_p.dist.ppf(.9999)\n",
    "    uniform_x = x_range[0] + u01 * (x_range[1] - x_range[0])\n",
    "    uniform_ln_x = np.exp(np.log(x_range[0]) + u01 * (np.log(x_range[1]) - np.log(x_range[0])))\n",
    "    return {\n",
    "        k: {\n",
    "            'size': v['x'],\n",
    "            'mult': np.round(v['y'] * params_p.norm).astype(int),\n",
    "        }\n",
    "        for k,v in\n",
    "        {\n",
    "            'sampling: uniform random in x': {\n",
    "                'x': uniform_x,\n",
    "                'y': params_p.dist.pdf(uniform_x) * (x_range[1] - x_range[0]) / params_p.n_part,\n",
    "            },\n",
    "            'sampling: uniform random in ln(x)': {\n",
    "                'x': uniform_ln_x,\n",
    "                'y': params_p.dist.pdf(uniform_ln_x) * (np.log(x_range[1]) - np.log(x_range[0])) / params_p.n_part * uniform_ln_x,\n",
    "            },\n",
    "            'sampling: constant multiplicity': {\n",
    "                'x': params_p.dist.ppf(u01),\n",
    "                'y': np.full(shape=params_p.n_part, fill_value=1 / params_p.n_part),\n",
    "            }\n",
    "        }.items()\n",
    "    }\n",
    "\n",
    "PARTICLES = sample(params_p=PARAMS_P, rng=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = {}\n",
    "for k in PARTICLES.keys():\n",
    "    B[k] = nx.DiGraph() #should it be a MultiDiGraph?\n",
    "    for i in range(PARAMS_P.n_part):\n",
    "        B[k].add_node(i, size_i=PARTICLES[k]['size'][i], mult_i=PARTICLES[k]['mult'][i])\n",
    "\n",
    "B['sampling: uniform random in x'].nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_solution(x, t):\n",
    "    \"\"\" for exponential initial condition \"\"\"\n",
    "    print(t)\n",
    "    tau = 1 - np.exp(-N0 * PARAMS_T.b * X0 * t)\n",
    "    sqrt_tau = np.sqrt(tau)\n",
    "    result = (\n",
    "        (1 - tau)\n",
    "        * 1\n",
    "        / (x * np.sqrt(tau))\n",
    "        * scipy.special.ive(1, 2 * x / X0 * sqrt_tau)\n",
    "        * np.exp(-(1 + tau - 2 * sqrt_tau) * x / X0)\n",
    "    )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(*, particles, params_p, params_t,rng, title='', time=0):\n",
    "    \"\"\" plots the particle state as both a histogram as well as population scatter plot\n",
    "    (with random coordinates shuffled for the purpose of plotting) \"\"\"\n",
    "    _, axs = pyplot.subplot_mosaic(\n",
    "        [['hist'], ['part']],\n",
    "        figsize=(11, 6),\n",
    "        sharex=True,\n",
    "        tight_layout=True,\n",
    "    )\n",
    "    u01 = rng.uniform(0, 1, params_p.n_part)\n",
    "    scale = params_p.norm / params_p.n_part\n",
    "    sampled_sizes = particles['sampling: constant multiplicity']['size']\n",
    "    for k in particles:\n",
    "        axs['hist'].hist(\n",
    "            x=np.log(particles[k]['size']),\n",
    "            weights=particles[k]['mult'] / (params_p.norm),\n",
    "            bins=64, # TODO!\n",
    "            range=(np.log(min(sampled_sizes)), np.log(max(sampled_sizes))),\n",
    "            label=f'{k}',\n",
    "            alpha=.666,\n",
    "            density=True,\n",
    "        )\n",
    "        axs['part'].scatter(\n",
    "            np.log(particles[k]['size']),\n",
    "            u01,\n",
    "            s=.25 + 2 * particles[k]['mult']/ scale\n",
    "        )\n",
    "    log_spaced_m = np.logspace(np.log(min(sampled_sizes)), np.log(max(sampled_sizes)), 256, base=np.e)\n",
    "    pdf_y = params_p.dist.pdf(log_spaced_m[:-1])\n",
    "    dm = np.diff(log_spaced_m)\n",
    "    dlogm = np.diff(np.log(log_spaced_m))   \n",
    "\n",
    "    axs['hist'].plot(np.log(log_spaced_m[:-1]), dm / dlogm * pdf_y , color='black', label='initial (SciPy)')\n",
    "    axs['hist'].plot(np.log(log_spaced_m[:-1]), dm / dlogm * analytic_solution(log_spaced_m[:-1], t=time), label='Golovin analytic', ls=\":\")\n",
    "    \n",
    "    axs['hist'].legend()\n",
    "    axs['hist'].set_ylabel('TODO pdf(x) ⋅ Δx [1]')\n",
    "    axs['hist'].set_title(title or f'{params_p.n_part=}   ')\n",
    "    m_ticks = np.array([1e-15, 1e-14, 1e-13])\n",
    "    axs['part'].set_xticks(np.log(m_ticks), m_ticks)\n",
    "    axs['part'].xaxis.set_tick_params(rotation=75)\n",
    "    axs['part'].set_xlabel('size')\n",
    "    axs['part'].set_yticks([])\n",
    "    axs['part'].set_ylim(0,1)\n",
    "    for axes in axs.values():\n",
    "        axes.grid()\n",
    "    show_plot()\n",
    "\n",
    "plot(particles=PARTICLES, params_p=PARAMS_P,params_t = PARAMS_T, rng=RNG, time=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_collisions = {}\n",
    "count_cloud_scavenging = {}\n",
    "count_scavenging = {}\n",
    "for k in PARTICLES.keys():\n",
    "    count_collisions[k] = 0.\n",
    "    count_cloud_scavenging[k] = 0.\n",
    "    count_scavenging[k] = 0.\n",
    "\n",
    "def coagulate(*, params_p, params_t, particles, rng,graphs):\n",
    "    \"\"\" performs Monte-Carlo coagulation of the particles using a simplified SDM algorithm for an additive kernel \"\"\"\n",
    "    n_pairs = params_p.n_part // 2\n",
    "    p_scale = params_p.n_part * (params_p.n_part - 1) / 2 / n_pairs\n",
    "    for _ in range(params_t.n_step):        \n",
    "        non_overlapping_pairs = rng.permutation(params_p.n_part)[: 2 * n_pairs].reshape(-1, 2)\n",
    "        u01 = rng.uniform(0, 1, n_pairs)\n",
    "        for samp,part in particles.items():\n",
    "            for alpha, pair in enumerate(non_overlapping_pairs):\n",
    "                j, k = pair\n",
    "                if part['mult'][j] < part['mult'][k]:\n",
    "                    j, k = k, j\n",
    "                kern = params_t.b * (part['size'][j] + part['size'][k])\n",
    "                prob = part['mult'][j] * kern * params_t.step / params_t.dv\n",
    "\n",
    "                gamma = np.ceil(prob * p_scale - u01[alpha])\n",
    "                if gamma != 0:\n",
    "                    gamma_t = min(gamma, part['mult'][j] // part['mult'][k])\n",
    "                    deficit = (gamma - gamma_t) * part['mult'][k]\n",
    "                    count_collisions[samp] += 1.\n",
    "                    # create edge in graph from j to k with weight gamma\n",
    "                    graphs[samp].add_edge(j, k, mult_transfer = gamma_t*part['mult'][k],\n",
    "                                          total_mass_transfer = gamma_t*part['size'][j]*part['mult'][k],\n",
    "                                          size_transfer = part['size'][j],\n",
    "                                          total_mass_deficit=deficit*part['size'][j],\n",
    "                                          size_deficit=deficit*part['size'][j] /part['mult'][k],\n",
    "                                          mult_deficit=deficit)\n",
    "\n",
    "                    if min(part['size'][j], part['size'][k]) < 4e-14:\n",
    "                        count_scavenging[samp] +=1\n",
    "                        if max(part['size'][j], part['size'][k]) > 5.2e-13:\n",
    "                            count_cloud_scavenging[samp] += 1.\n",
    "\n",
    "                    if part['mult'][j] > gamma_t * part['mult'][k]: \n",
    "                        part['mult'][j] -= gamma_t * part['mult'][k]\n",
    "                        part['size'][k] += gamma_t * part['size'][j] \n",
    "                    else:\n",
    "                        part['mult'][j] = part['mult'][k] // 2\n",
    "                            # assert part['mult'][j] != 0\n",
    "                        part['mult'][k] -= part['mult'][k] // 2\n",
    "                        part['size'][k] += gamma_t * part['size'][j]\n",
    "                        part['size'][j] = part['size'][k]\n",
    "\n",
    "coagulate(particles=PARTICLES, params_t=PARAMS_T, params_p=PARAMS_P, rng=RNG, graphs = B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add final size as node attribute\n",
    "for k in PARTICLES.keys():\n",
    "    for i in range(PARAMS_P.n_part):\n",
    "        B[k].nodes[i]['size_f'] = PARTICLES[k]['size'][i]\n",
    "        B[k].nodes[i]['mult_f'] = PARTICLES[k]['mult'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:check analytic solution\n",
    "plot(\n",
    "    particles=PARTICLES,\n",
    "    params_p=PARAMS_P,\n",
    "    params_t=PARAMS_T,\n",
    "    rng=RNG,\n",
    "    time=PARAMS_T.span,\n",
    "    title=f'  {PARAMS_T.span=}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_types = [\"total_mass\",\"size\",\"mult\"]\n",
    "\n",
    "fig, axs = pyplot.subplot_mosaic(\n",
    "    [['sampling: uniform random in x' + t for t in graph_types], \n",
    "        ['sampling: uniform random in ln(x)' + t for t in graph_types], \n",
    "        ['sampling: constant multiplicity' + t for t in graph_types]],\n",
    "    figsize=(9, 9),\n",
    "    tight_layout=True,\n",
    ")\n",
    "titles = {\n",
    "        'sampling: uniform random in x': 'uniform-in-r',\n",
    "        'sampling: uniform random in ln(x)': 'uniform-in-log(r)',\n",
    "        'sampling: constant multiplicity': 'constant-multiplicity',\n",
    "        'total_mass': 'Total Mass',\n",
    "        'mult': 'Multiplicity',\n",
    "        'size': 'Individual Mass',\n",
    "}\n",
    "\n",
    "for k in B:\n",
    "    pos = nx.forceatlas2_layout(B[k], scaling_ratio=0.2, strong_gravity=True, )\n",
    "\n",
    "    for graph_type in graph_types:\n",
    "        ax = axs[k+graph_type]\n",
    "\n",
    "        if graph_type == \"total_mass\":\n",
    "            attr_i = np.array([B[k].nodes[i]['size_i'] * B[k].nodes[i]['mult_i'] for i in B[k].nodes])\n",
    "            attr_f = np.array([B[k].nodes[i]['size_f'] * B[k].nodes[i]['mult_f']for i in B[k].nodes])\n",
    "            scale = 1e4\n",
    "            scale_edge = 1e2\n",
    "        else:\n",
    "            attr_i = np.array([B[k].nodes[i][graph_type + \"_i\"] for i in B[k].nodes])\n",
    "            attr_f = np.array([B[k].nodes[i][graph_type + \"_f\"] for i in B[k].nodes])\n",
    "            scale = 1e14 if graph_type == \"size\" else 1e-9\n",
    "            scale_edge = 1e4 if graph_type == \"size\" else 1e-11\n",
    "\n",
    "\n",
    "\n",
    "        nx.draw(B[k], pos, ax=ax, \n",
    "                node_size= scale * attr_i, \n",
    "                # edge width is mass_transfer\n",
    "                alpha= 0.4,\n",
    "                width=[0.2 + scale_edge * B[k].edges[e][graph_type +'_transfer'] for e in B[k].edges],\n",
    "                arrowsize=5,\n",
    "\n",
    "                with_labels=False\n",
    "                )\n",
    "        nx.draw_networkx_edges(B[k],pos=pos,ax=ax,\n",
    "                edgelist=[e for e in B[k].edges if B[k].edges[e]['mult_deficit']>0],\n",
    "                edge_color='red',\n",
    "                width=[0.2 + 1e-11 * B[k].edges[e]['mult_transfer'] for e in B[k].edges if B[k].edges[e]['mult_deficit']>0],\n",
    "                label='deficit',\n",
    "                #     arrowstyle=\"->\",\n",
    "                arrowsize=5,\n",
    "                )\n",
    "        nx.draw_networkx_nodes(B[k], pos, ax=ax, \n",
    "                node_size= scale * attr_f, #* B[k].nodes[i]['size_f'] * \n",
    "                node_color='purple',\n",
    "                alpha=0.3,\n",
    "                label='final size'\n",
    "                )\n",
    "        if k == 'sampling: uniform random in x':\n",
    "            ax.set_title(titles[graph_type], fontsize=12)\n",
    "\n",
    "        if graph_type == \"total_mass\":\n",
    "            ax.annotate(titles[k], xy=(-0.15, 0.5), xycoords='axes fraction', va='center', \n",
    "                        fontsize=12,\n",
    "                        rotation=90)\n",
    "\n",
    "pyplot.savefig('sdm_network.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
